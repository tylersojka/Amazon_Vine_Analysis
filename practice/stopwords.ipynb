{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stopwords","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMLmHMFFGjDAiqlKGBrxTuk"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18sXZu86X7GV","executionInfo":{"status":"ok","timestamp":1610031880617,"user_tz":360,"elapsed":36067,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}},"outputId":"e3ef3162-82f5-433c-f1ff-711fe5fb5d68"},"source":["import os\n","# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.0.1'\n","spark_version = 'spark-3.0.1'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n","Get:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n","Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [42.7 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [66.5 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [14.9 kB]\n","Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,707 kB]\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [247 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [53.3 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,274 kB]\n","Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [874 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [277 kB]\n","Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,140 kB]\n","Get:25 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,846 kB]\n","Get:26 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,376 kB]\n","Fetched 11.2 MB in 13s (866 kB/s)\n","Reading package lists... Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"COnFCvScYD4Z","executionInfo":{"status":"ok","timestamp":1610031891615,"user_tz":360,"elapsed":8687,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}}},"source":[" # Start Spark session\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"StopWords\").getOrCreate()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7CGiS98VYOKC","executionInfo":{"status":"ok","timestamp":1610032057370,"user_tz":360,"elapsed":6118,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}},"outputId":"86ed8b56-afcd-4d3e-8922-23b9f53c80ac"},"source":["# create dataframe\n","sentenceData = spark.createDataFrame([\n","    (0, ['Big', 'data', 'is', 'super', 'powerful']),\n","    (1, ['This', 'is', 'going', 'to', 'be', 'epic'])\n","], ['id', 'raw'])\n","sentenceData.show(truncate= False)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["+---+--------------------------------+\n","|id |raw                             |\n","+---+--------------------------------+\n","|0  |[Big, data, is, super, powerful]|\n","|1  |[This, is, going, to, be, epic] |\n","+---+--------------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GFlGdl_9Y3Pj","executionInfo":{"status":"ok","timestamp":1610037209081,"user_tz":360,"elapsed":359,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}}},"source":["#import stop words library\n","from pyspark.ml.feature import StopWordsRemover\n","# import tokenizer library\n","from pyspark.ml.feature import Tokenizer\n","# to create udf count column\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import IntegerType\n","from pyspark.ml import Pipeline"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"aoWT5BVAZBj_","executionInfo":{"status":"ok","timestamp":1610032150711,"user_tz":360,"elapsed":488,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}}},"source":["# run the remover\n","remover = StopWordsRemover(inputCol= \"raw\", outputCol= \"filtered\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iz3vnx7QZPaa","executionInfo":{"status":"ok","timestamp":1610032185192,"user_tz":360,"elapsed":1362,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}},"outputId":"f2cf4221-b258-4dd6-e326-33a46da7276c"},"source":["# transform and show data\n","remover.transform(sentenceData).show(truncate = False)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["+---+--------------------------------+----------------------------+\n","|id |raw                             |filtered                    |\n","+---+--------------------------------+----------------------------+\n","|0  |[Big, data, is, super, powerful]|[Big, data, super, powerful]|\n","|1  |[This, is, going, to, be, epic] |[going, epic]               |\n","+---+--------------------------------+----------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ohmzJWypZXnp","executionInfo":{"status":"ok","timestamp":1610037592728,"user_tz":360,"elapsed":373,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}}},"source":["# tokenize and remove stop words on df\n","# create the df\n","df = spark.createDataFrame([\n","    (0, \"We are going to now tokenize and remove stop words from this dataframe\"),\n","    (1, \"First we have break these sentences up into tokens, or single words\"),\n","    (3, \"Then we will remove all of the stop words from the sentences\")                       \n","], [\"id\", \"sentence\"])\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"KbFmzd3Xe_cA","executionInfo":{"status":"ok","timestamp":1610034136646,"user_tz":360,"elapsed":569,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}}},"source":["# create the tokenizer\n","tokenizer = Tokenizer(inputCol= \"sentence\", outputCol= \"words\")"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYUfNwGtfa3R","executionInfo":{"status":"ok","timestamp":1610034137827,"user_tz":360,"elapsed":372,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}}},"source":["# Create the remover\n","remover = StopWordsRemover(inputCol= \"words\", outputCol= \"filtered\")"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjfMUS6_sa57","executionInfo":{"status":"ok","timestamp":1610037595800,"user_tz":360,"elapsed":355,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}}},"source":["# create pipeline\n","pipeline = Pipeline(stages=[tokenizer, remover])"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlJRUK81sva8","executionInfo":{"status":"ok","timestamp":1610038215933,"user_tz":360,"elapsed":970,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}},"outputId":"c74c8604-1802-42af-fe0a-2c59803b56f1"},"source":["model = pipeline.fit(df)\n","df2 = model.transform(df)\n","df2.show(truncate= False)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["+---+----------------------------------------------------------------------+------------------------------------------------------------------------------------+-------------------------------------------------+\n","|id |sentence                                                              |words                                                                               |filtered                                         |\n","+---+----------------------------------------------------------------------+------------------------------------------------------------------------------------+-------------------------------------------------+\n","|0  |We are going to now tokenize and remove stop words from this dataframe|[we, are, going, to, now, tokenize, and, remove, stop, words, from, this, dataframe]|[going, tokenize, remove, stop, words, dataframe]|\n","|1  |First we have break these sentences up into tokens, or single words   |[first, we, have, break, these, sentences, up, into, tokens,, or, single, words]    |[first, break, sentences, tokens,, single, words]|\n","|3  |Then we will remove all of the stop words from the sentences          |[then, we, will, remove, all, of, the, stop, words, from, the, sentences]           |[remove, stop, words, sentences]                 |\n","+---+----------------------------------------------------------------------+------------------------------------------------------------------------------------+-------------------------------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3AfNLN_fqUM","executionInfo":{"status":"ok","timestamp":1610034263861,"user_tz":360,"elapsed":872,"user":{"displayName":"Tyler Sojka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjuSAFevUJa1gRZWtVga-ccYOTrGdu2lTxCmKUzcw=s64","userId":"11402974615608992588"}},"outputId":"8c302a26-5a8d-4d7c-a635-31bb84c4aedc"},"source":["df = tokenizer.transform(df)\n","df = remover.transform(df)\n","df.show(truncate= False)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["+---+----------------------------------------------------------------------+------------------------------------------------------------------------------------+-------------------------------------------------+\n","|id |sentence                                                              |words                                                                               |filtered                                         |\n","+---+----------------------------------------------------------------------+------------------------------------------------------------------------------------+-------------------------------------------------+\n","|0  |We are going to now tokenize and remove stop words from this dataframe|[we, are, going, to, now, tokenize, and, remove, stop, words, from, this, dataframe]|[going, tokenize, remove, stop, words, dataframe]|\n","|1  |First we have break these sentences up into tokens, or single words   |[first, we, have, break, these, sentences, up, into, tokens,, or, single, words]    |[first, break, sentences, tokens,, single, words]|\n","|3  |Then we will remove all of the stop words from the sentences          |[then, we, will, remove, all, of, the, stop, words, from, the, sentences]           |[remove, stop, words, sentences]                 |\n","+---+----------------------------------------------------------------------+------------------------------------------------------------------------------------+-------------------------------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aWd_WoDxg9JJ"},"source":[""],"execution_count":null,"outputs":[]}]}